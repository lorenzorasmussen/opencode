#!/bin/bash

# Advanced Code Refactoring Orchestration Script
# Handles automated analysis, technical debt tracking, and intelligent improvements

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Helper functions
print_header() {
    echo -e "${BLUE}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ${NC}"
}

print_success() {
    echo -e "${GREEN}‚úÖ $1${NC}"
}

print_error() {
    echo -e "${RED}‚ùå $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"
}

print_info() {
    echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"
}

print_metric() {
    echo -e "${CYAN}üìä $1${NC}"
}

print_debt() {
    echo -e "${PURPLE}üí∏ $1${NC}"
}

# Check if we're in a git repository
check_git_repo() {
    if [ ! -d ".git" ]; then
        print_error "Not a Git repository"
        echo "   Run 'git init' first"
        exit 1
    fi
}

# Count lines of code (optimized)
count_loc() {
    local dir="${1:-.}"
    local ext="${2:-*.ts,*.js,*.py}"

    if [[ "$ext" == "*"* ]]; then
        # Multiple extensions - use ripgrep for speed
        local patterns=""
        IFS=',' read -ra EXT_ARRAY <<< "$ext"
        for ext_pattern in "${EXT_ARRAY[@]}"; do
            patterns="$patterns -g '$ext_pattern'"
        done
        # Use wc -l on all matching files at once
        eval "find '$dir' $patterns -type f -print0 2>/dev/null | xargs -0 wc -l 2>/dev/null | tail -1 | awk '{print \$1}'" || echo 0
    else
        find "$dir" -name "$ext" -type f -print0 2>/dev/null | xargs -0 wc -l 2>/dev/null | tail -1 | awk '{print $1}' || echo 0
    fi
}

# Count files
count_files() {
    local dir="${1:-.}"
    local ext="${2:-*.ts,*.js,*.py}"

    if [[ "$ext" == "*"* ]]; then
        # Multiple extensions
        IFS=',' read -ra EXT_ARRAY <<< "$ext"
        local count=0
        for ext_pattern in "${EXT_ARRAY[@]}"; do
            count=$((count + $(find "$dir" -name "$ext_pattern" -type f | wc -l)))
        done
        echo $count
    else
        find "$dir" -name "$ext" -type f | wc -l
    fi
}

# Calculate cyclomatic complexity (simple approximation)
calculate_complexity() {
    local file="$1"
    local ext="${file##*.}"

    case "$ext" in
        ts|js)
            # Count control flow statements
            local if_count=$(grep -c '\bif\s*\(' "$file" 2>/dev/null || echo 0)
            local for_count=$(grep -c '\bfor\s*\(' "$file" 2>/dev/null || echo 0)
            local while_count=$(grep -c '\bwhile\s*\(' "$file" 2>/dev/null || echo 0)
            local switch_count=$(grep -c '\bswitch\s*\(' "$file" 2>/dev/null || echo 0)
            local catch_count=$(grep -c '\bcatch\s*\(' "$file" 2>/dev/null || echo 0)
            local complexity=$((if_count + for_count + while_count + switch_count + catch_count + 1))
            echo $complexity
            ;;
        py)
            # Simple Python complexity
            local if_count=$(grep -c '^\s*if\s' "$file" 2>/dev/null || echo 0)
            local for_count=$(grep -c '^\s*for\s' "$file" 2>/dev/null || echo 0)
            local while_count=$(grep -c '^\s*while\s' "$file" 2>/dev/null || echo 0)
            local try_count=$(grep -c '^\s*try\s*:' "$file" 2>/dev/null || echo 0)
            local complexity=$((if_count + for_count + while_count + try_count + 1))
            echo $complexity
            ;;
        *)
            echo 1
            ;;
    esac
}

# Analyze file for potential issues
analyze_file() {
    local file="$1"
    local issues=()

    # Check file size
    local lines=$(wc -l < "$file" 2>/dev/null || echo 0)
    if [ "$lines" -gt 500 ] 2>/dev/null; then
        issues+=("Large file: ${lines} lines (>500)")
    fi

    # Check complexity
    local complexity=$(calculate_complexity "$file")
    if [ "$complexity" -gt 15 ] 2>/dev/null; then
        issues+=("High complexity: $complexity (>15)")
    fi

    # Check for TODO/FIXME
    local todos=$(grep -c -i "todo\|fixme\|hack\|xxx" "$file" 2>/dev/null || echo 0)
    if [ "$todos" -gt 0 ] 2>/dev/null; then
        issues+=("$todos TODO/FIXME comments")
    fi

    # Check for console.log (in JS/TS)
    if [[ "$file" =~ \.(js|ts)$ ]]; then
        local console_logs=$(grep -c "console\." "$file" 2>/dev/null || echo 0)
        if [ "$console_logs" -gt 0 ] 2>/dev/null; then
            issues+=("$console_logs console statements")
        fi
    fi

    # Return issues as string
    if [ ${#issues[@]} -gt 0 ]; then
        echo "${issues[*]}"
    else
        echo "Clean"
    fi
}

# Command: analyze
cmd_analyze() {
    print_header "üîç Code Quality Analysis"

    check_git_repo

    echo ""
    print_info "Analyzing codebase..."

    # Basic metrics - use cached approach
    echo -n "Counting files... "
    local all_files=$(find . -name "*.ts" -o -name "*.js" -o -name "*.py" | grep -v node_modules | grep -v .opencode)
    local ts_files=$(echo "$all_files" | grep -c '\.ts$')
    local js_files=$(echo "$all_files" | grep -c '\.js$')
    local py_files=$(echo "$all_files" | grep -c '\.py$')
    local total_files=$((ts_files + js_files + py_files))

    echo -n "Counting lines... "
    local total_loc=0
    local ts_loc=0
    local js_loc=0
    local py_loc=0

    # Batch line counting for efficiency
    while read -r file; do
        local lines=$(wc -l < "$file" 2>/dev/null || echo 0)
        total_loc=$((total_loc + lines))
        case "$file" in
            *.ts) ts_loc=$((ts_loc + lines)) ;;
            *.js) js_loc=$((js_loc + lines)) ;;
            *.py) py_loc=$((py_loc + lines)) ;;
        esac
    done <<< "$all_files"

    print_metric "Files: $total_files total ($ts_files TS, $js_files JS, $py_files PY)"
    print_metric "Lines: $total_loc total ($ts_loc TS, $js_loc JS, $py_loc PY)"

    # Analyze largest files
    echo ""
    print_info "Largest files (>100 lines):"
    echo "$all_files" | while read file; do
        local lines=$(wc -l < "$file" 2>/dev/null || echo 0)
        if [ "$lines" -gt 100 ]; then
            printf "%4d %s\n" "$lines" "$file"
        fi
    done | sort -nr | head -10 | while read lines file; do
        printf "  %4d lines: %s\n" "$lines" "$file"
    done

    # Analyze complexity (limit to first 20 files for speed)
    echo ""
    print_info "High complexity files (>15):"
    echo "$all_files" | head -20 | while read file; do
        local complexity=$(calculate_complexity "$file")
        if [ "$complexity" -gt 15 ]; then
            printf "  %2d complexity: %s\n" "$complexity" "$file"
        fi
    done

    # Check for potential issues (limit to first 10 files for speed)
    echo ""
    print_info "Files with potential issues (sample):"
    echo "$all_files" | head -10 | while read file; do
        local issues=$(analyze_file "$file")
        if [ "$issues" != "Clean" ]; then
            echo "  $file: $issues"
        fi
    done

    echo ""
    print_success "Analysis complete"
}

# Command: debt-tracker
cmd_debt_tracker() {
    print_header "üí∏ Technical Debt Tracker"

    check_git_repo

    echo ""
    print_info "Analyzing technical debt..."

    # Get all relevant files once
    local all_files=$(find . -name "*.ts" -o -name "*.js" -o -name "*.py" | grep -v node_modules | grep -v .opencode)

    # Count large files efficiently
    local large_files=0
    while read -r file; do
        local lines=$(wc -l < "$file" 2>/dev/null || echo 0)
        if [ "$lines" -gt 500 ]; then
            large_files=$((large_files + 1))
        fi
    done <<< "$all_files"

    # Count complex files (sample first 20)
    local complex_files=0
    echo "$all_files" | head -20 | while read file; do
        local complexity=$(calculate_complexity "$file")
        if [ "$complexity" -gt 15 ]; then
            complex_files=$((complex_files + 1))
        fi
    done

    # Count total lines
    local total_loc=0
    while read -r file; do
        total_loc=$((total_loc + $(wc -l < "$file" 2>/dev/null || echo 0)))
    done <<< "$all_files"

    local debt_days=$(( (large_files * 2) + (complex_files * 3) + (total_loc / 1000) ))

    print_debt "Estimated Technical Debt: $debt_days days"
    print_debt "Large files (>500 lines): $large_files"
    print_debt "Complex files (>15 complexity): $complex_files (sampled)"
    print_debt "Total lines of code: $total_loc"

    echo ""
    print_info "Debt breakdown:"
    echo "  - Large files: $((large_files * 2)) days"
    echo "  - Complex files: $((complex_files * 3)) days (sampled)"
    echo "  - Code volume: $((total_loc / 1000)) days"

    if [ "$debt_days" -gt 20 ]; then
        print_warning "High debt level - consider refactoring"
    elif [ "$debt_days" -gt 10 ]; then
        print_info "Moderate debt level - monitor closely"
    else
        print_success "Low debt level - good job!"
    fi
}

# Command: suggest
cmd_suggest() {
    local target="${1:-.}"

    print_header "üí° Refactoring Suggestions"

    check_git_repo

    echo ""
    print_info "Analyzing $target for refactoring opportunities..."

    if [ -f "$target" ]; then
        # Analyze single file
        local lines=$(wc -l < "$target")
        local complexity=$(calculate_complexity "$target")
        local issues=$(analyze_file "$target")

        echo "File: $target"
        echo "Lines: $lines"
        echo "Complexity: $complexity"
        echo "Issues: $issues"
        echo ""

        # Generate suggestions
        if [ "$lines" -gt 500 ]; then
            print_warning "Suggestion: Split large file into smaller modules"
            echo "  - Extract classes/functions into separate files"
            echo "  - Apply Single Responsibility Principle"
        fi

        if [ "$complexity" -gt 15 ]; then
            print_warning "Suggestion: Reduce cyclomatic complexity"
            echo "  - Extract methods to reduce nesting"
            echo "  - Use early returns to simplify logic"
            echo "  - Consider strategy pattern for complex conditionals"
        fi

        if [[ "$issues" == *"TODO"* ]]; then
            print_info "Suggestion: Address TODO comments"
            echo "  - Either implement or remove outdated TODOs"
        fi

    elif [ -d "$target" ]; then
        # Analyze directory
        print_info "Top refactoring candidates in $target:"

        find "$target" -name "*.ts" -o -name "*.js" -o -name "*.py" | head -10 | while read file; do
            if [[ "$file" != "./node_modules/"* ]] && [[ "$file" != "./.opencode/"* ]]; then
                local lines=$(wc -l < "$file")
                local complexity=$(calculate_complexity "$file")

                if [ "$lines" -gt 300 ] || [ "$complexity" -gt 10 ]; then
                    echo "  $file (${lines} lines, complexity: $complexity)"
                fi
            fi
        done
    else
        print_error "Target not found: $target"
        exit 1
    fi

    echo ""
    print_success "Analysis complete"
}

# Command: report
cmd_report() {
    print_header "üìä Refactoring Report"

    check_git_repo

    echo ""
    print_info "Generating comprehensive refactoring report..."

    # Get all relevant files once
    local all_files=$(find . -name "*.ts" -o -name "*.js" -o -name "*.py" | grep -v node_modules | grep -v .opencode)

    # Calculate metrics efficiently
    local total_files=$(echo "$all_files" | wc -l)
    local total_loc=0
    local large_files=0
    local complex_files=0

    # Single pass through files
    while read -r file; do
        local lines=$(wc -l < "$file" 2>/dev/null || echo 0)
        total_loc=$((total_loc + lines))

        if [ "$lines" -gt 500 ]; then
            large_files=$((large_files + 1))
        fi
    done <<< "$all_files"

    # Count complex files (sample first 50 files for performance)
    echo "$all_files" | head -50 | while read file; do
        local complexity=$(calculate_complexity "$file")
        if [ "$complexity" -gt 15 ]; then
            complex_files=$((complex_files + 1))
        fi
    done

    # Calculate debt
    local debt_days=$(( (large_files * 2) + (complex_files * 3) + (total_loc / 1000) ))

    echo "EXECUTIVE SUMMARY"
    echo "================="
    print_metric "Total Files: $total_files"
    print_metric "Total Lines: $total_loc"
    print_debt "Technical Debt: $debt_days days"
    echo ""

    echo "DEBT BREAKDOWN"
    echo "=============="
    print_debt "Large Files (>500 lines): $large_files files"
    print_debt "Complex Files (>15 complexity): $complex_files files (sampled)"
    print_debt "Code Volume: $((total_loc / 1000)) days"
    echo ""

    echo "PRIORITY RECOMMENDATIONS"
    echo "========================"

    if [ "$large_files" -gt 0 ]; then
        print_warning "HIGH: Refactor large files"
        echo "  - Split monolithic files into smaller modules"
        echo "  - Apply Single Responsibility Principle"
    fi

    if [ "$complex_files" -gt 0 ]; then
        print_warning "HIGH: Reduce code complexity"
        echo "  - Extract methods to reduce cyclomatic complexity"
        echo "  - Use design patterns for complex logic"
    fi

    if [ "$debt_days" -gt 20 ]; then
        print_error "CRITICAL: High technical debt detected"
        echo "  - Immediate refactoring required"
        echo "  - Consider dedicated refactoring sprint"
    fi

    echo ""
    echo "NEXT STEPS"
    echo "=========="
    echo "1. Run './refactor <file> --suggest' for specific recommendations"
    echo "2. Address high-priority items first"
    echo "3. Consider automated refactoring tools"
    echo "4. Implement pre-commit hooks to prevent new debt"

    echo ""
    print_success "Report generated"
}

# Main command dispatcher
case "$1" in
    --analyze)
        cmd_analyze
        ;;
    --debt-tracker|--debt)
        cmd_debt_tracker
        ;;
    --suggest)
        cmd_suggest "$2"
        ;;
    --report)
        cmd_report
        ;;
    *)
        echo "Advanced Code Refactoring Orchestration"
        echo ""
        echo "Usage:"
        echo "  ./refactor --analyze              Run comprehensive code analysis"
        echo "  ./refactor --debt-tracker         Track technical debt metrics"
        echo "  ./refactor --suggest <target>     Get refactoring suggestions"
        echo "  ./refactor --report               Generate detailed report"
        echo ""
        echo "Examples:"
        echo "  ./refactor --analyze"
        echo "  ./refactor --suggest src/services/"
        echo "  ./refactor --report > refactoring-plan.md"
        echo ""
        echo "Targets can be files or directories"
        exit 1
        ;;
esac