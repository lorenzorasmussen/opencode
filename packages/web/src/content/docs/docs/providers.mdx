---
title: Providers
description: Using any LLM provider in opencode.
---

opencode uses the [AI SDK](https://ai-sdk.dev/) and [Models.dev](https://models.dev) to support for **75+ LLM providers** and it supports running local models.

To add a provider you need to:

1. Add the API keys for the provider using `opencode auth login`.
2. Configure the provider in your opencode config.

---

### Credentials

When you add a provider's API keys with `opencode auth login`, they are stored
in `~/.local/share/opencode/auth.json`.

---

### Config

You can customize the providers through the `provider` section in your opencode
config.

---

#### Base URL

You can customize the base URL for any provider by setting the `baseURL` option. This is useful when using proxy services or custom endpoints.

```json title="opencode.json" {6}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "anthropic": {
      "options": {
        "baseURL": "https://api.anthropic.com/v1"
      }
    }
  }
}
```

---

## Troubleshooting

### Credential Loop Issue

If you're stuck in a credential loop when adding a custom provider (like Cerebras), this usually happens when the provider configuration is incomplete. Here's how to fix it:

1. **Complete the auth setup**: Make sure you've successfully added the credential using `opencode auth login` and selected **Other**.

2. **Configure the provider**: The credential alone isn't enough. You must also configure the provider in your `opencode.json` file with the correct `npm` package, `baseURL`, and models.

3. **Use the correct npm package**: 
   - For Cerebras: Use `@ai-sdk/cerebras` (not `@ai-sdk/openai-compatible`)
   - For most other OpenAI-compatible providers: Use `@ai-sdk/openai-compatible`

4. **Example working configuration**:
   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "model": "cerebras/qwen-3-235b-a22b",
     "provider": {
       "cerebras": {
         "npm": "@ai-sdk/cerebras",
         "name": "Cerebras",
         "options": {
           "baseURL": "https://api.cerebras.ai/v1"
         },
         "models": {
           "qwen-3-235b-a22b": {
             "name": "Qwen-3-235b-a22b"
           }
         }
       }
     }
   }
   ```

### Global vs Local Configuration

opencode looks for configuration in this order:

1. **Local config**: `./opencode.json` in your current project directory
2. **Global config**: `~/.local/share/opencode/auth.json` (credentials only)

:::tip
If you want to use a provider globally, add the provider configuration to a local `opencode.json` file in each project where you want to use it. The credentials from the global auth file will be automatically used.
:::

**Best practice**: 
- Store credentials globally using `opencode auth login`
- Store provider configurations locally in each project's `opencode.json`

### Common Configuration Mistakes

1. **Wrong npm package**: Using `@ai-sdk/openai-compatible` when a specific package exists (like `@ai-sdk/cerebras`)

2. **Missing baseURL**: Always include the correct API endpoint in the `options.baseURL` field

3. **Incomplete model configuration**: Make sure to define at least one model in the `models` object

4. **API key format**: Different providers use different API key prefixes:
   - OpenAI: `sk-...`
   - Cerebras: `csk-...`
   - Anthropic: Uses OAuth (no manual key needed)

---

## Directory

Let's look at some of the providers in detail. If you'd like to add a provider to the
list, feel free to open a PR.

:::note
Don't see a provider here? Submit a PR.
:::

---

### Amazon Bedrock

To use Amazon Bedrock with opencode:

1. Head over to the **Model catalog** in the Amazon Bedrock console and request
   access to the models you want.
   
   :::tip
   You need to have access to the model you want in Amazon Bedrock.
   :::

1. You'll need either to set one of the following environment variables:

   - `AWS_ACCESS_KEY_ID`: You can get this by creating an IAM user and generating
     an access key for it.
   - `AWS_PROFILE`: First login through AWS IAM Identity Center (or AWS SSO) using
     `aws sso login`. Then get the name of the profile you want to use.
   - `AWS_BEARER_TOKEN_BEDROCK`: You can generate a long-term API key from the
   Amazon Bedrock console.
   
   Once you have one of the above, set it while running opencode.
   
   ```bash
   AWS_ACCESS_KEY_ID=XXX opencode
   ```
   
   Or add it to a `.env` file in the project root.
   
   ```bash title=".env"
   AWS_ACCESS_KEY_ID=XXX
   ```
   
   Or add it to your bash profile.
   
   ```bash title="~/.bash_profile"
   export AWS_ACCESS_KEY_ID=XXX
   ```

2. Run the `/models` command to select the model you want.

---

### Anthropic

We recommend signing up for [Claude Pro](https://www.anthropic.com/news/claude-pro) or [Max](https://www.anthropic.com/max), it's the most cost-effective way to use opencode.

Once you've singed up, run `opencode auth login` and select Anthropic.

```bash
$ opencode auth login

┌  Add credential
│
◆  Select provider
│  ● Anthropic (recommended)
│  ○ OpenAI
│  ○ Google
│  ...
└
```

This will ask you login with your Anthropic account in your browser. Now all the
the Anthropic models should be available when you use the `/models` command.

---

### GitHub Copilot

To use your GitHub Copilot subscription with opencode:

:::note
Some models might need a [Pro+
subscription](https://github.com/features/copilot/plans) to use.
:::

1. Run `opencode auth login` and select GitHub Copilot.

   ```bash
   $ opencode auth login
   ┌  Add credential
   
   │
   ◇  Select provider
   │  GitHub Copilot
   │
   ◇   ──────────────────────────────────────────────╮
   │                                                 │
   │  Please visit: https://github.com/login/device  │
   │  Enter code: 8F43-6FCF                          │
   │                                                 │
   ├─────────────────────────────────────────────────╯
   │
   ◓  Waiting for authorization...
   ```

2. Navigate to [github.com/login/device](https://github.com/login/device) and enter the code.

3. Now run the `/models` command to select the model you want.

---

### Groq

1. Head over to the [Groq console](https://console.groq.com/), click **Create API Key**, and copy the key.

2. Run `opencode auth login` and select Groq.

   ```bash
   $ opencode auth login

   ┌  Add credential
   │
   ◆  Select provider
   │  ● Groq
   │  ...
   └
   ```
   
3. Enter the API key for the provider.

   ```bash
   $ opencode auth login

   ┌  Add credential
   │
   ◇  Select provider
   │  Groq
   │
   ◇  Enter your API key
   │  _
   └
   ```

4. Run the `/models` command to select the one you want.

---

### LM Studio

You can configure opencode to use local models through LM Studio.

```json title="opencode.json" "lmstudio" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "lmstudio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (local)",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1"
      },
      "models": {
        "google/gemma-3n-e4b": {
          "name": "Gemma 3n-e4b (local)"
        }
      }
    }
  }
}
```

In this example:

- `lmstudio` is the custom provider ID. This can be any string you want.
- `npm` specifies the package to use for this provider. Here, `@ai-sdk/openai-compatible` is used for any OpenAI-compatible API.
- `name` is the display name for the provider in the UI.
- `options.baseURL` is the endpoint for the local server.
- `models` is a map of model IDs to their configurations. The model name will be displayed in the model selection list.

---

### Ollama

You can configure opencode to use local models through Ollama.

```json title="opencode.json" "ollama" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (local)",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "llama2": {
          "name": "Llama 2"
        }
      }
    }
  }
}
```

In this example:

- `ollama` is the custom provider ID. This can be any string you want.
- `npm` specifies the package to use for this provider. Here, `@ai-sdk/openai-compatible` is used for any OpenAI-compatible API.
- `name` is the display name for the provider in the UI.
- `options.baseURL` is the endpoint for the local server.
- `models` is a map of model IDs to their configurations. The model name will be displayed in the model selection list.

---

### OpenAI

https://platform.openai.com/api-keys

1. Head over to the [OpenAI Platform console](https://platform.openai.com/api-keys), click **Create new secret key**, and copy the key.

2. Run `opencode auth login` and select OpenAI.

   ```bash
   $ opencode auth login

   ┌  Add credential
   │
   ◆  Select provider
   │  ● OpenAI
   │  ...
   └
   ```
   
3. Enter the API key for the provider.

   ```bash
   $ opencode auth login

   ┌  Add credential
   │
   ◇  Select provider
   │  OpenAI
   │
   ◇  Enter your API key
   │  _
   └
   ```

4. Run the `/models` command to select the one you want.

---

### OpenRouter

1. Head over to the [OpenRouter dashboard](https://openrouter.ai/settings/keys), click **Create API Key**, and copy the key.

2. Run `opencode auth login` and select OpenRouter.

   ```bash
   $ opencode auth login

   ┌  Add credential
   │
   ◆  Select provider
   │  ● OpenRouter
   │  ○ Anthropic
   │  ○ Google
   │  ...
   └
   ```
   
3. Enter the API key for the provider.

   ```bash
   $ opencode auth login

   ┌  Add credential
   │
   ◇  Select provider
   │  OpenRouter
   │
   ◇  Enter your API key
   │  _
   └
   ```

4. Many OpenRouter models are preloaded by default, run the `/models` command to select the one you want.

   You can also add additional models through your opencode config.
   
   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "somecoolnewmodel": {},
         }
       }
     }
   }
   ```

5. You can also customize them through your opencode config. Here's an example of specifying a provider
   
   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "moonshotai/kimi-k2": {
             "options": {
               "provider": {
                 "order": ["baseten"],
                 "allow_fallbacks": false
               }
             }
           }
         }
       }
     }
   }
   ```

---

### Cerebras

Cerebras offers fast inference with generous free tiers and competitive pricing.

1. Head over to the [Cerebras console](https://inference.cerebras.ai/), create an account, and generate an API key.

2. Run `opencode auth login` and select **Other**.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

3. Enter `cerebras` as the provider ID.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◇  Select provider
   │  Other
   │
   ◇  Enter provider id
   │  cerebras
   └
   ```

4. Enter your Cerebras API key.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◇  Enter your API key
   │  csk-...
   └
   ```

5. Configure Cerebras in your opencode config.

   ```json title="opencode.json" "cerebras" {5-19}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "cerebras": {
         "npm": "@ai-sdk/cerebras",
         "name": "Cerebras",
         "options": {
           "baseURL": "https://api.cerebras.ai/v1"
         },
         "models": {
           "qwen-3-235b-a22b": {
             "name": "Qwen-3-235b-a22b"
           },
           "llama-3.3-70b": {
             "name": "Llama-3.3-70b"
           }
         }
       }
     }
   }
   ```

6. Run the `/models` command to select a Cerebras model.

---

### DeepSeek

DeepSeek offers powerful reasoning models at competitive prices.

1. Head over to the [DeepSeek console](https://platform.deepseek.com/), create an account, and generate an API key.

2. Run `opencode auth login` and select **Other**.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

3. Enter `deepseek` as the provider ID.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◇  Select provider
   │  Other
   │
   ◇  Enter provider id
   │  deepseek
   └
   ```

4. Enter your DeepSeek API key.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

5. Configure DeepSeek in your opencode config.

   ```json title="opencode.json" "deepseek" {5-17}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "deepseek": {
         "npm": "@ai-sdk/openai-compatible",
         "name": "DeepSeek",
         "options": {
           "baseURL": "https://api.deepseek.com/v1"
         },
         "models": {
           "deepseek-reasoner": {
             "name": "DeepSeek Reasoner"
           }
         }
       }
     }
   }
   ```

6. Run the `/models` command to select a DeepSeek model.

---

### Moonshot AI (Kimi)

Moonshot AI offers the Kimi models with long context capabilities.

1. Head over to the [Moonshot AI console](https://platform.moonshot.cn/), create an account, and generate an API key.

2. Run `opencode auth login` and select **Other**.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

3. Enter `moonshot` as the provider ID.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◇  Select provider
   │  Other
   │
   ◇  Enter provider id
   │  moonshot
   └
   ```

4. Enter your Moonshot API key.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

5. Configure Moonshot in your opencode config.

   ```json title="opencode.json" "moonshot" {5-17}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "moonshot": {
         "npm": "@ai-sdk/openai-compatible",
         "name": "Moonshot AI",
         "options": {
           "baseURL": "https://api.moonshot.cn/v1"
         },
         "models": {
           "kimi-k2-0711-preview": {
             "name": "Kimi K2"
           }
         }
       }
     }
   }
   ```

6. Run the `/models` command to select a Kimi model.

---

### Custom

To add any **OpenAI-compatible** provider that's not listed in `opencode auth login`:

:::tip
You can use any OpenAI-compatible provider with opencode. Most modern AI providers offer OpenAI-compatible APIs.
:::

#### Step 1: Add Credentials

1. Run `opencode auth login` and scroll down to **Other**.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

2. Enter a unique ID for the provider (use lowercase, no spaces).

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ◇  Enter provider id
   │  myprovider
   └
   ```

   :::note
   Choose a memorable ID - you'll use this in your config file.
   :::

3. Enter your API key for the provider.

   ```bash
   $ opencode auth login
   
   ┌  Add credential
   │
   ▲  This only stores a credential for myprovider - you will need configure it in opencode.json, check the docs for examples.
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

#### Step 2: Configure Provider

Create or update your `opencode.json` file in your project directory:

```json title="opencode.json" "myprovider" {5-15}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "myprovider": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "My AI Provider",
      "options": {
        "baseURL": "https://api.myprovider.com/v1"
      },
      "models": {
        "my-model-name": {
          "name": "My Model Display Name"
        }
      }
    }
  }
}
```

#### Configuration Options

| Field | Description | Required |
|-------|-------------|----------|
| `npm` | AI SDK package to use | ✅ |
| `name` | Display name in UI | ✅ |
| `options.baseURL` | API endpoint URL | ✅ |
| `models` | Available models | ✅ |
| `options.apiKey` | API key (if not using auth) | ❌ |

#### Common Examples

**Together AI:**
```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "together": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Together AI",
      "options": {
        "baseURL": "https://api.together.xyz/v1"
      },
      "models": {
        "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo": {
          "name": "Llama 3.2 11B Vision"
        }
      }
    }
  }
}
```

**Fireworks AI:**
```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "fireworks": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Fireworks AI",
      "options": {
        "baseURL": "https://api.fireworks.ai/inference/v1"
      },
      "models": {
        "accounts/fireworks/models/llama-v3p1-70b-instruct": {
          "name": "Llama 3.1 70B"
        }
      }
    }
  }
}
```

**Local API (with custom headers):**
```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "local": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Local API",
      "options": {
        "baseURL": "http://localhost:8000/v1",
        "headers": {
          "Authorization": "Bearer custom-token"
        }
      },
      "models": {
        "local-model": {
          "name": "Local Model"
        }
      }
    }
  }
}
```

#### Step 3: Select Model

Run the `/models` command to select your custom provider's model:

```bash
$ opencode
> /models
```

Your custom provider and models will appear in the selection list.

#### Tips

- **Provider ID**: Use lowercase letters and hyphens only (e.g., `my-provider`)
- **Model names**: Use the exact model ID from the provider's API documentation
- **Base URL**: Always include the full path (usually ending in `/v1`)
- **Testing**: Start with one model to test the configuration before adding more

